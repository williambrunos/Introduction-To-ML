{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/williambrunos/Introduction-To-ML/blob/main/Class_5/Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks\n",
        "\n",
        "Neural networks is a computacional way to mimic the way that human beings learn things. To understand that, we need to understand how a neuron work."
      ],
      "metadata": {
        "id": "ano3BVg_EiVS"
      },
      "id": "ano3BVg_EiVS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How dows neurons works? (Overly Simplified)\n",
        "\n",
        "A **neuron** is the main cell of the **neuro system**. It is composed of **dendrites**, wich ones recieves informations from other neurons trough eletrical signs. If these eletrical signs trespasses a certain threshold, the **nucleus** of the neuron allows the passage of this information recieved trough the tail of the neuron and to the **axons**, which passes these eletrical signs to other neurons."
      ],
      "metadata": {
        "id": "3G1TW-VXE54d"
      },
      "id": "3G1TW-VXE54d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Neuron](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Neuron.svg/1200px-Neuron.svg.png)"
      ],
      "metadata": {
        "id": "qeHQUah2Hpnf"
      },
      "id": "qeHQUah2Hpnf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, imagine that a certain information needs to be processed. For doing this, our neuron system sends some informations to a certain neuron layer, which are capable of understanding and extracting patterns of this informations.\n",
        "\n",
        "These processed and with a certain complexity informations are passed through the axons to other neurons, wich are capable of processing and undertanding patterns even more complexes than the other informations from the previous layer.\n",
        "\n",
        "And this process keeps going on, until the system has the full information processed.\n",
        "\n",
        "So...how does it works artificialy? Let's see!"
      ],
      "metadata": {
        "id": "zHcvgCI3HZ-G"
      },
      "id": "zHcvgCI3HZ-G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Artificial Neuron\n",
        "\n",
        "Those eletrical signals recieved by the neurons on neuro system can be represented artificially by input values $x_i$, which one represents a certain information comming to be processed by the neuron. Each information has a certain weight $w_i$, which represents how strong is the presence of the input  $x_i$ for the calculus of the information.\n",
        "\n",
        "These inputs can either be excitatory or inhibitory. Inhibitory inputs are those that have maximum effect on the decision making irrespective of other inputs i.e., if x_3 is 1 (not home) then my output will always be 0 i.e., the neuron will never fire, so x_3 is an inhibitory input. Excitatory inputs are NOT the ones that will make the neuron fire on their own but they might fire it when combined together.\n",
        "\n",
        "The weighted sum of the inputs for the neurons are done with a certain bias $b_0$:\n",
        "\n",
        "$$v = \\sum_{i=1}^{m}(x_i w_i + b_0)$$\n",
        "\n",
        "The value of v could represent our processed information, but this would simplify a lot our system architecture, because our info would be represented by a linear function, which does not needs any complex architecture, wich is the case of neural networks.\n",
        "\n",
        "So, the value of $v$ is passed as an argument for a function called the **activation function** $\\phi(v)$, that do a more complex transformation on the information than a simple linear transformation. The output of this function is the resultant transformed information called $y$, which can be used by other neurons in the next layer or can be the resultant info.\n",
        "\n",
        "![Artificial Neuron](https://www.gabormelli.com/RKB/images/thumb/3/31/artificial-neuron-model.png/600px-artificial-neuron-model.png)\n",
        "\n",
        "And the output of the processes is passed on to the next layers in a hierarchical manner, some of the neurons will fire and some wonâ€™t and this process goes on until it results in a final response."
      ],
      "metadata": {
        "id": "vTqpf52cKLO_"
      },
      "id": "vTqpf52cKLO_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Perceptron\n",
        "\n",
        "The perceptron is the most simple example of a artificial neuron, wich has the output based on the function below:\n",
        "\n",
        "$$y = 1, \\; if \\; \\sum_{i=1}^{m}(x_iw_i + b_0) \\ge 0$$\n",
        "$$y = 0, \\; if \\; \\sum_{i=1}^{m}(x_iw_i + b_0) < 0 $$\n",
        "\n",
        "**obs**: The McCulloch Pitts Neuron didn't had the weights, being just a sum of the input values to outuput the function. \n",
        "\n",
        "See more: [McCulloch Pitts Neuron](https://towardsdatascience.com/mcculloch-pitts-model-5fdf65ac5dd1)\n",
        "\n",
        "Why does not we use the McCulloch Pitts Neuron on nowdays? Because it is a very simplistic way to model a system. It needs fixed thresholds and gives the same importance to all the inputs, and we want a model that learns the weights of each input and the thresholds!"
      ],
      "metadata": {
        "id": "m1B5-jQ8M9HP"
      },
      "id": "m1B5-jQ8M9HP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi Layer Perceptron (MLP)\n",
        "\n",
        "Notice that one unique perceptron would be very simplistic with the problem. So, we are going to use various layers of perceptrons, each on with one or more than one perceptron.\n",
        "\n",
        "![MLP](https://www.researchgate.net/publication/334609713/figure/fig1/AS:783455927406593@1563801857102/Multi-Layer-Perceptron-MLP-diagram-with-four-hidden-layers-and-a-collection-of-single.jpg)"
      ],
      "metadata": {
        "id": "RPOo0OwqWmR7"
      },
      "id": "RPOo0OwqWmR7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks Implementation with Scikit-Learn"
      ],
      "metadata": {
        "id": "o0TnJGh7aLBT"
      },
      "id": "o0TnJGh7aLBT"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e68beaac",
      "metadata": {
        "id": "e68beaac"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "import numpy as np \n",
        "\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_boston"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = load_boston(return_X_y=True)"
      ],
      "metadata": {
        "id": "gi7ObAeeasTR",
        "outputId": "5f3f6da6-35f8-4451-edd8-6079e3e23462",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "gi7ObAeeasTR",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)"
      ],
      "metadata": {
        "id": "Rdsq8rkfbH8X"
      },
      "id": "Rdsq8rkfbH8X",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On neural networks, we use learning based on optimizers. Because of this, we have to standardize the values on the dataframe. Doing that, the model will not priorize some inputs rather than others just because the values are not in the same range of values."
      ],
      "metadata": {
        "id": "SVmS2OjMcVj9"
      },
      "id": "SVmS2OjMcVj9"
    },
    {
      "cell_type": "code",
      "source": [
        "mm = MinMaxScaler()"
      ],
      "metadata": {
        "id": "9Z7sTDIcbN4o"
      },
      "id": "9Z7sTDIcbN4o",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = mm.fit_transform(X_train)\n",
        "X_test = mm.transform(X_test)"
      ],
      "metadata": {
        "id": "5ME30j8UdBWe"
      },
      "id": "5ME30j8UdBWe",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLPRegressor(hidden_layer_sizes=(500, 500, 500), max_iter=1000, solver='lbfgs', shuffle=False)"
      ],
      "metadata": {
        "id": "AOgJIfNUdGl3"
      },
      "id": "AOgJIfNUdGl3",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "l82VMzmUec1w",
        "outputId": "941ccbf7-6a40-460f-9b99-a7832551a0f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        }
      },
      "id": "l82VMzmUec1w",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-fba0f5670a4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mMLP\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \"\"\"\n\u001b[0;32m--> 752\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_solver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"lbfgs\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             self._fit_lbfgs(\n\u001b[0;32m--> 441\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m             )\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit_lbfgs\u001b[0;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units)\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0;34m\"gtol\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             },\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         )\n\u001b[1;32m    549\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_optimize_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lbfgs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 610\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    336\u001b[0m         _lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr,\n\u001b[1;32m    337\u001b[0m                        \u001b[0mpgtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miwa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsave\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                        isave, dsave, maxls)\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0mtask_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'FG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "rIcaO0svefYz",
        "outputId": "4a0dabe4-2903-4142-9c67-fa50f3e2c93b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rIcaO0svefYz",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8414872938393994"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Neural_Networks.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}